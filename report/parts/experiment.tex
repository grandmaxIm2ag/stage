\section{Experiment}

\subsection{Data}
To experiment our algorithm we use the dataset 20Newsgroups \cite{Newsgroups20}.
The 20 Newsgroups data set is a collection of approximately 20,000 Newsgroups 
documents, partitioned evenly across 20 different newsgroups. We use also the 
RCV1 dataset \cite{Lewis:2004:RNB:1005332.1005345}. The RCV1 dataset is a 
collection of over 800,000 text documents. For RCV1 dataset, we use only a 
subset of 10,000 documents from RCV1 such that each document belongs to only 
one of the root classes in the class hierarchy. This was detailed in 
[\cite{Deap-K-Means}].\\
Each document are represented by a vector using term frequency-inverse document 
frequency (TFIDF) representation~\cite{doi:10.1108/eb026526}.
The term frequency-inverse document frequency is a method of weighting depicting 
the significance of each word of a document in relative to a corpus.
\begin{equation}
TF(t, X) = \frac{f_{t, X}}{max_{t' \in C}f_{t', X}} 
\end{equation}
\begin{equation}
IDF(t, C) = log(\frac{N}{|X \in C : t \in X|})
\end{equation}
\begin{equation}
TFIDF(t,X,C) = TF(t, X) . IDF(t, C)   
\end{equation}
For each dataset there is a preprocessing step. We remove stopword and keep only
the 2000 words with the top TFIDF scores. We use also a stemming step 
\cite{journals/mtcl/Lovins68} for dataset 20 Newsgroups. 
\\We divide each dataset in two subset :
\begin{enumerate}
\item Validation Set : representing 10\% of total dataset use for hyperparameter optimization (see section\ref{seq:hyperparam}).
\item Test Set : representing 90\% of total dataset use to compare result from different algorithm (see section \ref{seq:results}).
\end{enumerate} 
\subsection{Generate Constraint}
To generate the set of keywords $KW$ we rank each word of each 
document of each class using TFIDF according to algorithm~\ref{algo:gen_kw}.
We add for each document of each class the TFIDF of each word, in this manner
we find the most important word of each class. Furthermore, for discriminativ keywords,
we substract  the TFIDF of other class, so that the keywords are the most discriminating.
\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{Corpus C, The number of keywords per classes $P$}
  \Output{KW}
  $KW \gets \{\}$\\
  \ForEach{Class $c_i \in C$}{
    $rank_i \gets [0 ... 0]$\\
    \ForEach{Document $X \in c_i$}{
      \ForEach{Word $w \in X$}{
        $rank_{i,w} \gets rank_{i,w} + TFIDF(w,X, C)$\\
      }
    }
  }
  \If{$Discriminating\_ Extraction$}{
    \ForEach{Class $c_i \in C$}{
      ${rank'}_i \gets rank_i - \sum\limits_{\forall c_j, c_j \neq c_i}rank_j$\\
    }
    $rank \gets rank'$
  }
  \ForEach{Class $c_i \in C$}{
    $KW \gets KW \cup \{\{w_1, w_2 ... w_P\} : \not\exists (v_1, v_2) | v_1 \not\in 
    \{w_1, w_2 ... w_P\}, v_2 \in \{w_1, w_2 ... w_P\}, rank_{i,v_1} \ge rank_{i,v_2}\}$\\
  }
  \Return{KW}
  \caption{\label{algo:gen_kw}Extract Keywords}
\end{algorithm}
%\subsubsection{Background Knowledge}
%We generate pairwise constraints randomly accordinf to algorithm
%~\ref{algo:gen_pair}
%\begin{algorithm}[!h]
%  \SetKwInOut{Input}{input}
%  \SetKwInOut{Output}{output}
%  \Input{Corpus C, The set of labels L, The number of pair $N_p$}
%  \Output{Must-Link Pair ML, Cannot-Link Pair CL}
%  \For{p = 1 : $N_p$}{
%    \Repeat{$X_i \neq X_j \wedge L_i = L_j$}{
%      Choose randomly ($X_i, X_j$)\\
%    }
%    Insert ($X_i, X_j$) in ML\\
%  }
%  \For{p = 1 : $N_p$}{
%    \Repeat{$X_i \neq X_j \wedge L_i \neq L_j$}{
%      Choose randomly ($X_i, X_j$)\\
%    }
%    Insert ($X_i, X_j$) in ML\\
%  }
%  \Return{ML, CL}
%  \caption{\label{algo:gen_pair}Extract Pair}
%\end{algorithm}
\subsection{Evaluation}
\subsubsection{Baseline Algorithm}
We evaluate our algorithm with $K$-Means and Deep $K$-Means with pretraining 
see in section~\ref{seq:DeepClust}.
\subsubsection{Metric}
To evaluate our algorithm and compare results with reference algorithms we can
use the NMI Metric, Accuracy Metric \cite{NMI_ACC}, and Adjusted
Rand index\cite{ARI}. 
\begin{itemize}
\item NMI is an information-theoretic measure based on the mutual information of the ground-truth classes
and the obtained clusters, normalized using the entropy of each. The NMI Metric is defined as follows
$$NMI(S,C) = \frac{I(S,C)}{[H(S)+H(C)]/2}$$ 
with
$I(S,C) =\sum\limits_k \sum\limits_f\frac{|s_k \cap c_f|}{N}log\frac{N|s_k \cap c_f|}{|s_k| |c_f|}$
and
$H(S) = -\sum\limits_k\frac{|s_k|}{N}log\frac{N|s_k|}{|s_k|}$
\item The Accuracy is the proportion of true results among the total
  number of cases examined. The Accuracy metric is defined as follows :
$$
ACC(S,C) = \frac{1}{N}\sum\limits_k {max}_j|s_k \cap c_j|
$$
\item Let a be the number of pairs of document in C
  that are in the same cluster in the predicted partition and in the
  same cluster in the real partition, and b be the number of pairs of
  document in C that are in different clusters in predicted partition
  and in different cluster in real partition.
  The Adjusted Rand index is defined as follows :
  $$ARI = \frac{a+b}{\binom{N}{2}}$$
\end{itemize}
When we add the additional class, metrics are computed only with
documents that do not belong to the additional class.
\subsection{Experimental Setup}
\subsubsection{Autoencoder Architecture}
We use the same architecture used in~\cite{Deap-K-Means}. The encoder is a fully-connected 
multilayer perceptron formed by 3 hidden layers (with dimensions 500, 500, 2000) 
and an embedding layer (with dimension K, the number of cluster). 
The decoder is a mirrored version of the encoder~\ref{fig:autoenc}.
The ReLu activation function is used on layers, except for the third
hidden layer of encoder and decoder part.
\subsubsection{\label{seq:hyperparam}Hyperparameters}
We used Gride Search strategy for $\lambda_0$ and $\lambda_1$ optimization.
We search on the range 
$[10^{-6},10^{-5}10^{-4},10^{-3},10^{-2},10^{-1}]$ for $\lambda_0$ and $\lambda_1$.   
We select hyperparameters which maximize the Accuracy Metrics for Validation Set.
The results of Gride Search are reported in table~\ref{tab:line_search_met1}.
\begin{table}
\caption{\label{tab:line_search_met1}Best results of Line Search for the optimization of
hyperparameters for each dataset for the Unclustered Keywords method.}
%The best result for each  metric/dataset is bold.}
\centering
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-2}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-4}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray} &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, LP sim   &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-2}$         \\ \hline
\end{tabular}
}
\subcaption*{RCV1}
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-1}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-2}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray}  &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-6}$  &$10^{-1}$         \\ \hline
       CDKM, LP sim   &$10^{-3}$  &$10^{-1}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-6}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-5}$         \\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups}
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-1}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-2}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray}  &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, LP sim   &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-4}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-5}$         \\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups without noise}
%\resizebox{.3\textwidth}{!}{
%\begin{tabular}{|l|l|l|}
%    \hline
%                      &$\lambda_0$&$\lambda_0$       \\ \hline
%    Deep $K$-Means    &$10^{-1}$  &$10^{-1}$         \\ \hline
%    AE + KM, LP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%    AE + KM, LP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, LP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, LP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, SP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, SP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%\end{tabular}
%}
%\subcaption*{DBPedia}
\end{table}

\subsection{\label{seq:results}Results}
The purpose of the experiment is to rediscover the different classes of 
datasets with keywords.\\
To add noise to 20 newsgroups dataset e divide the dataset into two corpus $C_1, C_2$. 
Each corpus contain ten classes. We generate keywords \ref{algo:gen_kw} 
from $C_1$. Then we concatenate document from corpus $C_1$ with document
from corpus $C_2$. The clustering processed on corpus $C_1$. For this experiment
we extract 3 keywords per class (see algorithm \ref{algo:gen_kw}).
\\Results for baseline are reported in table \ref{tab:res_baseline}.

\begin{table}
\caption{\label{tab:res_mask}Clustering applies to 
different learned latent space to measure the efficiency of lexical constraints
for $K$-Means algorithm. Performance is measured in terms of NMI, Adjusted Rand 
Index and clustering Accuracy, higher is better. Each cell contains the average
and the standard deviation computed over 10 runs.}
%The best result for each  metric/dataset is bold.}
\centering
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$48.8\pm6.6$&$18.4\pm6.0$&$29.7\pm5.8$\\ \hline
    AE + KM, SP       &$51.3\pm3.5$&$17.5\pm5.8$&$24.5\pm5.1$\\ \hline
    Deep $K$-Means    &$54.4\pm4.9$&$23.9\pm3.5$&$29.6\pm3.6$\\ \hline
    AE + KM, LP mask  &$65.0\pm6.7$&$35.6\pm7.3$&$37.5\pm6.5$\\ \hline
    AE + KM, LP sim   &$62.7\pm6.1$&$35.1\pm5.7$&$38.4\pm4.1$\\ \hline
       CDKM, LP mask  &\boldmath$72.7\pm4.0$&\boldmath$43.8\pm5.4$&\boldmath$44.1\pm3.9$\\ \hline
       CDKM, LP sim   &$72.5\pm4.5$&$43.7\pm5.7$&$44.0\pm4.3$\\ \hline
       CDKM, SP mask  &$70.3\pm4.2$&$41.0\pm5.1$&$42.1\pm4.0$\\ \hline
       CDKM, SP sim   &$70.4\pm5.1$&$41.8\pm7.1$&$43.0\pm5.2$\\ \hline
       
\end{tabular}
}
\subcaption*{RCV1}
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$36.1\pm2.2$&$13.3\pm1.7$&$40.9\pm1.6$\\ \hline
    AE + KM, SP       &$53.1\pm2.3$&$35.0\pm1.4$&$49.3\pm1.0$\\ \hline
    Deep $K$-Means    &$54.9\pm1.7$&$37.6\pm1.1$&$51.6\pm0.6$\\ \hline
    AE + KM, LP mask  &\boldmath$67.1\pm4.5$&$37.6\pm5.1$&$39.8\pm3.9$\\ \hline
    AE + KM, LP sim   &$56.0\pm2.3$&$37.6\pm1.6$&$50.8\pm0.7$\\ \hline
       CDKM, LP mask  &$61.3\pm0.6$&\boldmath$41.2\pm0.8$&$52.7\pm0.5$\\ \hline
       CDKM, LP sim   &$60.6\pm1.3$&$40.5\pm1.3$&$53.0\pm0.8$\\ \hline
       CDKM, SP mask  &$60.2\pm1.8$&\boldmath$41.2\pm0.9$&\boldmath$53.5\pm0.5$\\ \hline
       CDKM, SP sim   &$60.5\pm1.2$&$40.7\pm0.8$&$52.9\pm0.5$\\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups}
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$33.1\pm2.7$&$ 8.7\pm1.3$&$26.3\pm1.6$\\ \hline
    AE + KM, SP       &$45.2\pm3.3$&$23.0\pm2.5$&$30.0\pm2.0$\\ \hline
    Deep $K$-Means    &$44.5\pm2.4$&$23.6\pm2.4$&$30.2\pm2.0$\\ \hline
    AE + KM, LP mask  &$45.9\pm2.2$&$24.0\pm1.8$&$31.8\pm1.5$\\ \hline
    AE + KM, LP sim   &$46.2\pm2.6$&$24.6\pm1.9$&$32.3\pm1.3$\\ \hline
       CDKM, LP mask  &$50.0\pm2.1$&$26.0\pm1.6$&$31.7\pm1.7$\\ \hline
       CDKM, LP sim   &$49.4\pm2.1$&$25.3\pm1.9$&$31.1\pm1.7$\\ \hline
       CDKM, SP mask  &$49.5\pm1.7$&$26.1\pm1.1$&$32.0\pm0.9$\\ \hline
       CDKM, SP sim   &$49.3\pm1.1$&$25.9\pm1.2$&$32.0\pm1.1$\\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups with noise}
%\resizebox{\hsize}{!}{
%\begin{tabular}{|l|l|l|l|}
%    \hline
%                      & ACC        &ARI         &NMI       \\ \hline
%    $K$-Means         &$54.6\pm3.2$&$31.2\pm2.7$&$63.4\pm0.9$\\ \hline
%    AE + KM, SP       &$72.9\pm2.5$&$62.7\pm2.5$&$72.8\pm1.5$\\ \hline
%    Deep $K$-Means    &$73.6\pm4.8$&$63.5\pm2.9$&$74.6\pm1.3$\\ \hline
%    AE + KM, LP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%    AE + KM, LP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%  CDKM, LP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, LP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, SP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, SP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%\end{tabular}
%}
%\subcaption*{DBPedia}
\end{table}

\subsubsection{Non Discriminative Keywords}
\begin{table}
\caption{\label{tab:res_mask}Clustering applies to 
different learned latent space to measure the efficiency of lexical constraints
for $K$-Means algorithm. Performance is measured in terms of NMI, Adjusted Rand 
Index and clustering Accuracy, higher is better. Each cell contains the average
and the standard deviation computed over 10 runs.}
%The best result for each  metric/dataset is bold.}
\centering
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$48.8\pm6.6$&$18.4\pm6.0$&$29.7\pm5.8$\\ \hline
    Deep $K$-Means    &$54.4\pm4.9$&$23.9\pm3.5$&$29.6\pm3.6$\\ \hline
       CDKM, LP mask  &\boldmath$68.5\pm5.6$&\boldmath$39.6\pm6.4$&\boldmath$41.3\pm4.6$\\ \hline
       CDKM, LP sim   &$65.8\pm5.0$&$36.2\pm5.5$&$38.9\pm3.7$\\ \hline
       CDKM, SP mask  &$68.5\pm5.6$&$39.6\pm6.4$&$41.3\pm4.6$\\ \hline
       CDKM, SP sim   &$67.5\pm6.3$&$38.6\pm6.4$&$40.7\pm4.4$\\ \hline
\end{tabular}
}
\subcaption*{RCV1}
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$36.1\pm2.2$&$13.3\pm1.7$&$40.9\pm1.6$\\ \hline
    Deep $K$-Means    &$54.9\pm1.7$&$37.6\pm1.1$&$51.6\pm0.6$\\ \hline
       CDKM, LP mask  &$58.8\pm1.4$&$38.5\pm1.4$&$52.6\pm0.6$\\ \hline
       CDKM, LP sim   &\boldmath$60.1\pm2.1$&\boldmath$40.2\pm1.4$&\boldmath$52.8\pm1.0$\\ \hline
       CDKM, SP mask  &$58.9\pm2.0$&$38.9\pm1.6$&\boldmath$52.8\pm0.9$\\ \hline
       CDKM, SP sim   &$60.1\pm1.7$&$40.1\pm1.5$&$52.7\pm0.7$\\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups}
\resizebox{\hsize}{!}{
\begin{tabular}{|l|l|l|l|}
    \hline
                      & ACC      &ARI       &NMI       \\ \hline
    $K$-Means         &$33.1\pm2.7$&$ 8.7\pm1.3$&$26.3\pm1.6$\\ \hline
    Deep $K$-Means    &$44.5\pm2.4$&$23.6\pm2.4$&$30.2\pm2.0$\\ \hline
       CDKM, LP mask   &$48.8\pm2.3$&\boldmath$25.7\pm1.9$&\boldmath$31.6\pm1.9$\\ \hline
       CDKM, LP sim  &$47.8\pm2.6$&$24.3\pm1.8$&$30.8\pm1.7$\\ \hline
       CDKM, SP mask  &$48.8\pm2.3$&$25.7\pm1.9$&$31.6\pm1.9$\\ \hline
       CDKM, SP sim   &\boldmath$49.2\pm2.3$&$25.2\pm2.0$&$31.5\pm1.7$\\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups with noise}
%\resizebox{\hsize}{!}{
%\begin{tabular}{|l|l|l|l|}
%    \hline
%                      & ACC        &ARI         &NMI       \\ \hline
%    $K$-Means         &$54.6\pm3.2$&$31.2\pm2.7$&$63.4\pm0.9$\\ \hline
%    AE + KM, SP       &$72.9\pm2.5$&$62.7\pm2.5$&$72.8\pm1.5$\\ \hline
%    Deep $K$-Means    &$73.6\pm4.8$&$63.5\pm2.9$&$74.6\pm1.3$\\ \hline
%    AE + KM, LP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%    AE + KM, LP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%  CDKM, LP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, LP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, SP mask  &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%       CDKM, SP sim   &$00.0\pm0.0$&$00.0\pm0.0$&$00.0\pm0.0$\\ \hline
%\end{tabular}
%}
%\subcaption*{DBPedia}
\end{table}

\subsubsection{Robustness}

We can also test the robustness of our algorithm. To test this, we
can vary the number of keywords by classes. In addition we compare
the results of each version of CDKM.
We can see results  in 
figures \ref{fig:rcv1}, \ref{fig:20news}, \ref{fig:20news_noise}.
\begin{figure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/acc/RCV1_ACC.png}}     
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/ari/RCV1_ARI.png}}    
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/nmi/RCV1_NMI.png}}    
  \end{subfigure}
  \caption{\label{fig:rcv1}Results for RCV1 dataset}
\end{figure}

\begin{figure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/acc/20NEWS_ACC.png}}     
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/ari/20NEWS_ARI.png}}    
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/nmi/20NEWS_NMI.png}}    
  \end{subfigure}\caption{\label{fig:20news}Results for 20 newsgroups dataset}
\end{figure}

\begin{figure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/acc/20NEWS_noisy_ACC.png}}     
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/ari/20NEWS_noisy_ARI.png}}     
  \end{subfigure}
  \begin{subfigure}[b]{\hsize}
    \centering
    \fbox{\includegraphics[scale=0.82]{parts/res/dat_file/nmi/20NEWS_noisy_NMI.png}}
  \end{subfigure}\caption{\label{fig:20news_noise}Results for 20 newsgroups dataset with noise}
\end{figure}
