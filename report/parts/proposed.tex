\section{Integrating Lexical Constraints}
The idea is to learn a latent space taking into account lexical constraints and
background knowledge.
\\We denote  X a document of size d,
C the corpus, N the size of C, and K the number of cluster.
There are two ways to see this problem, the first is that the user gives a set 
of keywords and it's up to our algorithm to find the classes that best fit our 
set of keywords. A second way of seeing, is that the user gives keywords with 
classes, in other word he gives for each class a set of keywords.\\
We denote :
\begin{equation}
KW = \begin{pmatrix}KW_1  \\ ... \\ KW_k \\ ...\\ ... \\ KW_{K}\end{pmatrix}
\end{equation}
where $KW_k$ the set of keyword for the $k^{th}$ class. Also, we denote X' a biasing
version of X. X' can be defined in three different ways :
\begin{enumerate}
\item \textbf{Masked Document} :
\begin{equation}
X' = \begin{pmatrix}mask_1(X)  \\ ... \\ mask_k(X) \\ ...\\ ... \\ mask_K(X)\end{pmatrix}
\end{equation}
where $mask_k(X)$ is the document X masked by keywords from the $k^{th}$ class
\item \textbf{Similarity function} :
\begin{equation}
   X'_k = \forall i, X_i * \mu(i,k) 
\end{equation}
where $\mu(i,k)$ is a function brighting to light the similarity between a keyword $w$ and a word $i$.
\begin{equation}
  \mu(i,k) = max \left(\frac{rel(i,k)}{\sum\limits_{k' \in KW}rel(i,k')}, 0\right)
\end{equation}
where $r(i,k)$ is a function brighting to light the similarity between a keyword $w$ and a word $i$.
\begin{equation}
  rel(i,k) = \frac{1}{|k|} \sum\limits_{w \in k} s(w,i)
\end{equation}
where $s(w,i)$ is a function brighting to light the similarity between a keyword $w$ and a word $i$.
\begin{equation}\label{sim}
s(w,i) = \frac{df(w,i)}{df(w)}
\end{equation}
\end{enumerate}
\subsection{Lexical Constraints}
We want representation where document X is close of the closest
$X'_k$ : 
\begin{equation}
|| h_\theta(X) - h_\theta(c'(X ; X' )) ||_2^2
\end{equation}
where $c'(X ; X') = \argmin\limits_{X'_k \in X'} ||h_\theta(X) - h_\theta(X'_k) ||_2^2$\\ 
However, the  function $c'$ is not differentiable, and we cant use SGD algorithm
\cite{doi:10.1080/01621459.1982.10477894}
to learn this function. To approximate the $\argmin$ function we can use the 
parameterized softmax function \cite{doi:10.1117/1.2819119}. The softmax 
function can be used as a differentiable substitute to $\argmin$. 
\\The penalty takes the form : 
\begin{equation}\label{eq:omega_kw_soft}
\begin{array}{l}
  \sum\limits_{X \in C} \sum\limits_{k = 1}^K \frac{e^{-\alpha|| h_\theta(X) - 
h_\theta(X'_k)||_2^2}}{\sum\limits_{k' = 1}^K e^{-\alpha|| h_\theta(X) - 
h_\theta(X'_{k'})||_2^2}}|| h_\theta(X) - h_\theta(X'_{k})||_2^2
\end{array}
\end{equation}

\subsection{Deep $K$-Means}

For the Deep $K$-Means, we can use the approach proposed by Moradi Fard, Thonet and Gaussier 
\cite{Deap-K-Means} see in section 2.3.\\
For the clustering loss we can use squared loss with euclidean distance. We
denote $R = \begin{pmatrix} r_1 & r_2 & ... & r_K\end{pmatrix}$ the vector of
centroids.\\
Finally the loss function for Unclustered Keywords is :
\begin{equation}
L(C ,\alpha;\theta,R) = \sum\limits_{X \in C} ||X - A(X;\theta)||_2^2 + 
\lambda_0 \sum\limits_{X \in C}\sum\limits_{k=1}^K||h_\theta(X) - r_k ||_2^2 G_{k}(h_\theta(X), \alpha; R) + 
\lambda_1|| h_\theta(X) - h_\theta(X')||_2^2
\end{equation}
and the loss function for Clustered Keywords is :
\begin{equation}
\begin{array}{c c}
L(C ,\alpha;\theta,R) = & \sum\limits_{X \in C} ||X - A(X;\theta)||_2^2 + 
\lambda_0 \sum\limits_{X \in C}\sum\limits_{k=1}^K||h_\theta(X) - r_k ||_2^2 G_{k}(h_\theta(X), \alpha; R) + 
\\ & \lambda_1'\sum\limits_{X \in C} \sum\limits_{k = 1}^K \frac{e^{-\alpha|| h_\theta(X) - 
h_\theta(X'_k)||_2^2}}{\sum\limits_{k' = 1}^K e^{-\alpha|| h_\theta(X) - 
h_\theta(X'_{k'})||_2^2}}|| h_\theta(X) - h_\theta(X'_{k})||_2^2
\end{array}
\end{equation}
with hyperparameters $\lambda_0 \geq 0, \lambda_1 \geq 0, \lambda_1' \geq 0$.\\

\subsection{Learning Algorithm and Pretraining}

For the learning algorithm we can use the Deep $K$-Means algorithm with 
pretraining~(algorithm \ref{algo:dkm}) see in section \ref{seq:DeepClust}. 
 
\subsubsection{Pretraining}\label{sec:pre}
The pretraining we performed here simply consists in initializing the weights by training 
the auto-encoder then, petraining allows to initialize centroïds.
We use two methods for pretraining : 
\begin{enumerate} 
\item \textbf{simple pretrain} : minimizing only reconstruct loss :
  $\sum\limits_{X \in X} || X - f(h_\theta(X))||_2^2$. This allows to 
  initialize with the least loss of informations about documents.   
\item \textbf{lexical pretrain} : minimizing $\sum\limits_{X \in X} || X - f(h_\theta(X))||_2^2 + 
  \lambda_1 \sum\limits_{X \in C} || h_\theta(X) - h_\theta(X')||_2^2$. 
  It allows to initialize centroïds with bias representation.
\end{enumerate}

\subsubsection{Centers Initialisation}
After pretraining we need to initiallise centers. We denote $S^{(k)}$ as follow :
\begin{equation}
\forall i, S^{(k)}_i = \left\{
    \begin{array}{ll}
        log\left(\frac{N}{df(i)}\right) & \mbox{if } i \in KW \\
        0 & \mbox{Otherwise}
    \end{array}
\right.
\end{equation}
Then we initialise R as follows :
\begin{equation}
\forall k, r_k =  h_\theta(S^{(k)})
\end{equation}
