@article{doi:10.1117/1.2819119,
author = { Nasser M. Nasrabadi},
title = {Pattern Recognition and Machine Learning},
journal = {Journal of Electronic Imaging},
volume = {16},
year = {2007},
doi = {10.1117/1.2819119},
URL = {https://doi.org/10.1117/1.2819119}
}
@ARTICLE{2015arXiv151106321H,
   author = {{Hsu}, Y.-C. and {Kira}, Z.},
    title = "{Neural network-based clustering using pairwise constraints}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.06321},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106321H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{doi:10.1108/eb026526,
author = {KAREN SPARCK JONES},
title = {A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL},
journal = {Journal of Documentation},
volume = {28},
number = {1},
pages = {11-21},
year = {1972},
doi = {10.1108/eb026526},

URL = { 
        https://doi.org/10.1108/eb026526
    
},
eprint = { 
        https://doi.org/10.1108/eb026526
    
}
,
    abstract = { The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. }
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{Deap-K-Means,
    title={Deep K-Means : An End-to-End, Annealing-based Approach for jointly Clustering with K-Means and Learning Representations},
    author={Maziar Moradi Fard and Thibaut Thonet and Eric Gaussier},
    publisher = {Submitted to 32nd Conference on Neural Information Processing Systems (NIPS 2018)}
}

@inproceedings{Wagstaff:2001:CKC:645530.655669,
 author = {Wagstaff, Kiri and Cardie, Claire and Rogers, Seth and Schr\"{o}dl, Stefan},
 title = {Constrained K-means Clustering with Background Knowledge},
 booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
 series = {ICML '01},
 year = {2001},
 isbn = {1-55860-778-1},
 pages = {577--584},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645530.655669},
 acmid = {655669},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@ARTICLE{2018arXiv180107648A,
   author = {{Aljalbout}, E. and {Golkov}, V. and {Siddiqui}, Y. and {Cremers}, D.
	},
    title = "{Clustering with Deep Learning: Taxonomy and New Methods}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.07648},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, 62H30, 62M45, 91C20, H.3.3, I.2.6, I.5, I.5.3, I.5.4},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180107648A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{1056489,
author={S. Lloyd},
journal={IEEE Transactions on Information Theory},
title={Least squares quantization in PCM},
year={1982},
volume={28},
number={2},
pages={129-137},
keywords={Least-squares approximation;PCM communication;Quantization (signal);Signal quantization},
doi={10.1109/TIT.1982.1056489},
ISSN={0018-9448},
month={March},}
@misc{Newsgroups20,
added-at = {2012-05-12T16:39:37.000+0200},
author = {{empty}},
biburl = {https://www.bibsonomy.org/bibtex/2133313f1254d09b6ae89da442b62f3bc/lopusz_kdd},
editor = {{empty}},
interhash = {77445767e91f49215dc81ff8236ca9d5},
intrahash = {133313f1254d09b6ae89da442b62f3bc},
keywords = {dataset},
timestamp = {2012-05-12T18:24:34.000+0200},
title = { 20 Newsgroups Dataset},
url = {http://people.csail.mit.edu/jrennie/20Newsgroups/},
year = {{empty}}
}

@article{NMI_ACC,
title = "Locally consistent concept factorization for document clustering",
abstract = "Previous studies have demonstrated that document clustering performance can be improved significantly in lower dimensional linear subspaces. Recently, matrix factorization-based techniques, such as Nonnegative Matrix Factorization (NMF) and Concept Factorization (CF), have yielded impressive results. However, both of them effectively see only the global euclidean geometry, whereas the local manifold geometry is not fully considered. In this paper, we propose a new approach to extract the document concepts which are consistent with the manifold geometry such that each concept corresponds to a connected component. Central to our approach is a graph model which captures the local geometry of the document submanifold. Thus, we call it Locally Consistent Concept Factorization (LCCF). By using the graph Laplacian to smooth the document-to-concept mapping, LCCF can extract concepts with respect to the intrinsic manifold structure and thus documents associated with the same concept can be well clustered. The experimental results on TDT2 and Reuters-21578 have shown that the proposed approach provides a better representation and achieves better clustering results in terms of accuracy and mutual information.",
keywords = "clustering., concept factorization, graph Laplacian, manifold regularization, Nonnegative matrix factorization",
author = "Deng Cai and Xiaofei He and Jiawei Han",
year = "2011",
month = "3",
day = "15",
doi = "10.1109/TKDE.2010.165",
language = "English (US)",
volume = "23",
pages = "902--913",
journal = "IEEE Transactions on Knowledge and Data Engineering",
issn = "1041-4347",
publisher = "IEEE Computer Society",
number = "6",
}
 BibTeX | EndNote | ACM Ref

@article{ARI,
author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
title = {Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance},
journal = {J. Mach. Learn. Res.},
issue_date = {3/1/2010},
volume = {11},
month = dec,
year = {2010},
issn = {1532-4435},
pages = {2837--2854},
numpages = {18},
url = {http://dl.acm.org/citation.cfm?id=1756006.1953024},
acmid = {1953024},
publisher = {JMLR.org},
}
@article{Lewis:2004:RNB:1005332.1005345,
author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
title = {RCV1: A New Benchmark Collection for Text Categorization Research},
journal = {J. Mach. Learn. Res.},
issue_date = {12/1/2004},
volume = {5},
month = dec,
year = {2004},
issn = {1532-4435},
pages = {361--397},
numpages = {37},
url = {http://dl.acm.org/citation.cfm?id=1005332.1005345},
acmid = {1005345},
publisher = {JMLR.org},
}

@article{SWANN1969S39,
title = "A survey of non-linear optimization techniques",
journal = "FEBS Letters",
volume = "2",
pages = "S39 - S55",
year = "1969",
issn = "0014-5793",
doi = "https://doi.org/10.1016/0014-5793(69)80075-X",
url = "http://www.sciencedirect.com/science/article/pii/001457936980075X",
author = "W.H. Swann",
abstract = "Optimization means the provision of a set of numerical parameter values which will give the best fit of an equation, or series of equations, to a set of data. For simple systems this can be done by differentiating the equations with respect to each parameter in turn, setting the set of partial differential equations to zero, and solving this set of simultaneous equations (as for exwnple in linear regression). In more complicated cases, however, it may be impossible to differentiate the equations, or very difficultly soluble non-linear equations may result. Many numerical optimization techniques to overcome these difficulties have been developed in the least ten years, and this review explains the logical basis of most of them, without going into the detail of computational procedures. The methods fall naturally into two classes — direct search methods, in which only values of the function to be minimized (or maximized) are used — and gradient methods, which also use derivatives of the function. The author considers all the accepted methods in each class, although warning that gradient methods should not be used unless the analytical differentiation of the function to be minimized is possible. If the solution is constrained, that is, certain values of the parameters are regarded as impossible or certain relations between the parameter values must be obeyed, the problem is more difficult. The second part of the review considers methods which have been proposed for the solution of constrained optimization problems."
}
@article{kullback1951,
author = "Kullback, S. and Leibler, R. A.",
doi = "10.1214/aoms/1177729694",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "79--86",
publisher = "The Institute of Mathematical Statistics",
title = "On Information and Sufficiency",
url = "https://doi.org/10.1214/aoms/1177729694",
volume = "22",
year = "1951"
}
@article{doi:10.1162/089976604773135104,
author = {Rosasco, Lorenzo and Vito, Ernesto De and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
title = {Are Loss Functions All the Same?},
journal = {Neural Computation},
volume = {16},
number = {5},
pages = {1063-1076},
year = {2004},
doi = {10.1162/089976604773135104},

URL = {
        https://doi.org/10.1162/089976604773135104

},
eprint = {
        https://doi.org/10.1162/089976604773135104

}
,
    abstract = { In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage. }
}

@article{journals/mtcl/Lovins68,
added-at = {2017-05-23T00:00:00.000+0200},
author = {Lovins, Julie Beth},
biburl = {https://www.bibsonomy.org/bibtex/2226cae545c797b61100a789a01c297d2/dblp},
ee = {http://www.mt-archive.info/MT-1968-Lovins.pdf},
interhash = {1812bf3f2154cc09f318da089b027dc2},
intrahash = {226cae545c797b61100a789a01c297d2},
journal = {Mech. Translat. and Comp. Linguistics},
keywords = {dblp},
number = {1-2},
pages = {22-31},
timestamp = {2017-05-24T11:38:52.000+0200},
title = {Development of a stemming algorithm.},
url = {http://dblp.uni-trier.de/db/journals/mtcl/mtcl11.html#Lovins68},
volume = 11,
year = 1968
}

