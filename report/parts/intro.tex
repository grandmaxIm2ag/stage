\section{Introduction}\label{sec:intro}

Clustering is one of the most fundamental tasks in data mining and machine
learning. $K$-Means algorithm is a clustering method using untrod models,
it represents each cluster by a single mean vector. $K$-Means clustering sorts
n objects into k clusters in which each observation belongs to
the cluster with the nearest centroid.
We use because $K$-Means is often used in practice and it is easy to interpret 
the clustering results.
This problem is computationally
difficult (NP-hard).In real application domains, users may want to introduce 
constraints to finding 
useful properties for clustering data. The difficulty with integration of 
constraints into $K$-Means algorithm is to find a good representation for data 
taking into account constraints. The Deep Learning and Auto-Encoder can be used 
to learn this representation. With Auto-Encoder we have to perform the $K$-Means
in the latent space learned, and this latent space must be $K$-Means friendly.
\\In this study, we specifically focus on the k-Means algorithm with lexical constraints 
and background knowledge problem. Lexical constraints are represented
by a set of keywords given by the user. Background knowledge are the set of 
pairwise constraints given by the user.  We use an Auto-encoder to learn a latent space taking
into account constraints. The loss of the Auto-Encoder is divided in two parts, (a) 
the reconstruct loss $L_{rec}$ and (b) different penalties $\omega$ skewing the
representation. Then, the representation must be $K$-Means friendly, 
to do this, we use the Deep $K$-Means model \cite{Deap-K-Means}.  
\\In the next section, we provide some background on the $K$-Means algorithm and
deep learning. In section 3, we proposed a method to introduce constraints to 
the $K$-Means algorithm. And we are experimenting our method in section 4.
