\section{Experiment}\label{seq:exp}

\subsection{Data}
To experiment our algorithm we use the dataset 20Newsgroups \cite{Newsgroups20}.
The 20 Newsgroups data set is a collection of approximately 20,000 Newsgroups 
documents, partitioned evenly across 20 different newsgroups. We use also the 
RCV1 dataset \cite{Lewis:2004:RNB:1005332.1005345}. The RCV1 dataset is a 
collection of over 800,000 text documents. For RCV1 dataset, we use only a 
subset of 10,000 documents from RCV1 such that each document belongs to only 
one of the root classes in the class hierarchy. This was detailed in 
[\cite{Deap-K-Means}].\\
Each document are represented by a vector using term frequency-inverse document 
frequency (TFIDF) representation~\cite{doi:10.1108/eb026526}.
The term frequency-inverse document frequency is a method of weighting depicting 
the significance of each word of a document in relative to a corpus.
\begin{equation}
TF(t, X) = \frac{f_{t, X}}{max_{t' \in C}f_{t', X}} 
\end{equation}
\begin{equation}
IDF(t, C) = log(\frac{N}{|X \in C : t \in X|})
\end{equation}
\begin{equation}
TFIDF(t,X,C) = TF(t, X) . IDF(t, C)   
\end{equation}
For each dataset there is a preprocessing step. We remove stopword and keep only
the 2000 words with the top TFIDF scores. We use also a stemming step 
\cite{journals/mtcl/Lovins68}. 
\subsection{Keywords Exctraction}
To generate the set of keywords $KW$ we rank each word of each 
document of each class using TFIDF according to algorithm~\ref{algo:gen_kw}.
We add for each document of each class the TFIDF of each word, in this manner
we find the most important word of each class. Furthermore, for discriminative keywords,
we substract  the TFIDF of other class, so that the keywords are the most discriminating.
\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{Corpus C, The number of keywords per classes $P$}
  \Output{KW}
  $KW \gets \{\}$\\
  \ForEach{Class $c_i \in C$}{
    $rank_i \gets [0 ... 0]$\\
    \ForEach{Document $X \in c_i$}{
      \ForEach{Word $w \in X$}{
        $rank_{i,w} \gets rank_{i,w} + TFIDF(w,X, C)$\\
      }
    }
  }
  \If{$Discriminating\_ Extraction$}{
    \ForEach{Class $c_i \in C$}{
      ${rank'}_i \gets rank_i - \sum\limits_{\forall c_j, c_j \neq c_i}rank_j$\\
    }
    $rank \gets rank'$
  }
  \ForEach{Class $c_i \in C$}{
    $KW \gets KW \cup \{\{w_1, w_2 ... w_P\} : \not\exists (v_1, v_2) | v_1 \not\in 
    \{w_1, w_2 ... w_P\}, v_2 \in \{w_1, w_2 ... w_P\}, rank_{i,v_1} \ge rank_{i,v_2}\}$\\
  }
  \Return{KW}
  \caption{\label{algo:gen_kw}Extract Keywords}
\end{algorithm}
\subsection{Evaluation}
\subsubsection{Baseline Algorithm}
As our goal in this work is to study the k-Means clustering algorithm with 
constraints, we focus on the family of $K$-Means-related models and compare our 
approach against state-of-the-art models from this family, using both standard 
and deep clustering models. For the standard clustering methods, we used: the 
$K$-Means clustering approach, denoted \textbf{KM}; an approach denoted as \textbf{AE-KM} in which 
dimensional reduction is first performed using an auto-encoder followed by 
$K$-Means applied to the learned representations. For AE-KM, we can use only 
the reconstruct loss (denoted \textbf{AE-KM SP}) or integrate lexical constraints loss 
(denoted \textbf{AE-KM LP}).\\For the deep clustering models, we use the Deep $K$-Means 
Model see in section~\ref{seq:DeepClust} denoted \textbf{DKM} with pretraining.
\subsubsection{\label{seq:metric}Metric}
To evaluate our algorithm and compare results with reference algorithms we can
use the NMI Metric, Accuracy Metric \cite{NMI_ACC}, and Adjusted
Rand index\cite{ARI}. 
\begin{itemize}
\item NMI is an information-theoretic measure based on the mutual information of the ground-truth classes
and the obtained clusters, normalized using the entropy of each. The NMI Metric is defined as follows
$$NMI(S,C) = \frac{I(S,C)}{[H(S)+H(C)]/2}$$ 
with
$I(S,C) =\sum\limits_k \sum\limits_f\frac{|s_k \cap c_f|}{N}log\frac{N|s_k \cap c_f|}{|s_k| |c_f|}$
and
$H(S) = -\sum\limits_k\frac{|s_k|}{N}log\frac{N|s_k|}{|s_k|}$
\item The Accuracy is the proportion of true results among the total
  number of cases examined. The Accuracy metric is defined as follows :
$$
ACC(S,C) = \frac{1}{N}\sum\limits_k {max}_j|s_k \cap c_j|
$$
\item Let a be the number of pairs of document in C
  that are in the same cluster in the predicted partition and in the
  same cluster in the real partition, and b be the number of pairs of
  document in C that are in different clusters in predicted partition
  and in different cluster in real partition.
  The Adjusted Rand index is defined as follows :
  $$ARI = \frac{a+b}{\binom{N}{2}}$$
\end{itemize}
\subsection{Experimental Setup}
\subsubsection{Autoencoder Architecture}
We use the same architecture used in~\cite{Deap-K-Means}. The encoder is a fully-connected 
multilayer perceptron formed by 3 hidden layers (with dimensions 500, 500, 2000) 
and an embedding layer (with dimension K, the number of cluster). 
The decoder is a mirrored version of the encoder~\ref{fig:autoenc}.
All layers except the one preceding the embedding layer and the one
preceding the output layer are applied a ReLU activation function \cite{Nair:2010:RLU:3104322.3104425} 
before being fed to the next
layer. For the layer preceding the embedding layer and for the layer preceding the output layer
we apply the identity function.
\subsubsection{\label{seq:protocol}Experimental Protocol}
The purpose of the experiment is to rediscover the different classes of 
datasets with keywords.
\\To add noise to 20 newsgroups dataset we divide the dataset into two corpus 
$C_1, C_2$. Each corpus contains ten classes. We generate keywords 
\ref{algo:gen_kw} from $C_1$. Then we concatenate document from corpus $C_1$
with document from corpus $C_2$. The clustering processed on corpus $C_1$.
\\The tests were carried out in 3 steps :
\begin{enumerate}
\item Discriminative Keywords : We test our algorithm with 3 discriminative 
keywords (see  algorithm \ref{algo:gen_kw}).
\item Non Discriminative Keywords : We test our algorithm with 3 non dicriminative 
keywords (see  algorithm \ref{algo:gen_kw}).
\item Robustness : We test the robustness of our algorithm. To test this, we
can vary the number of keywords by classes. In addition we compare
the results of each version of CDKM. We generate discriminative keywords with 
algorithm \ref{algo:gen_kw} for this test.
\end{enumerate}
For all tests we select hyperparameters with 3 dicriminatives keywords per 
classes see section \ref{seq:results}. 
\subsubsection{\label{seq:hyperparam}Hyperparameters Selection}
The hyperparameters $\lambda_0$ and $\lambda_1$ , that define the trade-off 
between the lexical constraint error and clustering  error in the loss function, 
were determined by performing a grid search on the set \{$10^i | i \in [-6, -1]$\}.
To do so, we randomly split each dataset into a validation set (10\% of the 
data) and a test set (90\%). Each model is trained on the whole data and only 
the validation set labels are leveraged in the grid search to identify the 
optimal $\lambda_0$ and $\lambda_1$.
\\We select hyperparameters which maximize the Accuracy Metrics for Validation Set.
The results of Grid Search are reported in table~\ref{tab:line_search_met1}.
\begin{table}[!h]
\caption{\label{tab:line_search_met1}Best results of Grad Search for the optimization of
hyperparameters for each dataset.}
%The best result for each  metric/dataset is bold.}
\centering
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-2}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-4}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray} &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, LP sim   &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-2}$         \\ \hline
\end{tabular}
}
\subcaption*{RCV1}
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-1}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-2}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray}  &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-6}$  &$10^{-1}$         \\ \hline
       CDKM, LP sim   &$10^{-3}$  &$10^{-1}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-6}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-5}$         \\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups}
\resizebox{.3\textwidth}{!}{
\begin{tabular}{|l|l|l|}
    \hline
                      &$\lambda_0$&$\lambda_1$       \\ \hline
    Deep $K$-Means    &$10^{-1}$  &\cellcolor{gray}         \\ \hline
    AE + KM, LP mask  &\cellcolor{gray}  &$10^{-2}$         \\ \hline
    AE + KM, LP sim   &\cellcolor{gray}  &$10^{-2}$         \\ \hline
       CDKM, LP mask  &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, LP sim   &$10^{-1}$  &$10^{-3}$         \\ \hline
       CDKM, SP mask  &$10^{-1}$  &$10^{-4}$         \\ \hline
       CDKM, SP sim   &$10^{-1}$  &$10^{-5}$         \\ \hline
\end{tabular}
}
\subcaption*{20 Newsgroups without noise}
%\resizebox{.3\textwidth}{!}{
%\begin{tabular}{|l|l|l|}
%    \hline
%                      &$\lambda_0$&$\lambda_0$       \\ \hline
%    Deep $K$-Means    &$10^{-1}$  &$10^{-1}$         \\ \hline
%    AE + KM, LP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%    AE + KM, LP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, LP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, LP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, SP mask  &$10^{-1}$  &$10^{-1}$         \\ \hline
%       CDKM, SP sim   &$10^{-1}$  &$10^{-1}$         \\ \hline
%\end{tabular}
%}
%\subcaption*{DBPedia}
\end{table}
\subsection{\label{seq:results}Results}
To present the tests, we denote \textbf{CDKM MASK} (respectively \textbf{CDKM SIM}) the version of 
the algorithm using masked document (respectively similarity function), and \textbf{LP}
(respectively \textbf{SP}) when we use the Lexical Pretrain (respectively Simple Pretrain).
\\The results for the evaluation of the compared clustering methods on the 
different benchmark datasets are summarized in tables \ref{tab:res_mask} 
\ref{tab:res_non_discr} and in figures \ref{fig:rcv1}, \ref{fig:20news},
\ref{fig:20news_noise}. The clustering performance is evaluated with respect to
three standard measures Normalized Mutual Information (NMI), the Adjusted Rand 
Index (ari) and the clustering accuracy (ACC) see in section \ref{seq:metric}. 
We report for each dataset/method pair the average and standard deviation of 
these metrics computed over 10 runs and conduct significance testing as 
described in section \ref{seq:protocol}. Bold Result in each column of tables 
\ref{tab:res_mask} \ref{tab:res_non_discr} corresponds to the best result for 
the corresponding method/metric.
\subsubsection{Discriminative Keywords}
Results are reported in table \ref{tab:res_mask}.
\\We can observe that cdkm is always better than baselines, whatever the method 
to define X' and the type of pretraining. We also observe that Lexical Pretrain 
is generally better than Simple Pretrain. Finally, the results for the mask method
and for the sim method are similar.
\\The strongest progression is for the dataset RCV1, and the weakest progression 
is for the dataset 20 newsgroup with noise.
\\While observing results for \textbf{AE+KM mask LP} and \textbf{AE+KM sim LP},
we notice that lexical biases give better results than $K$-Means  or Deep 
$K$-Means.
\subsubsection{Non Discriminative Keywords}
Results are reported in table \ref{tab:res_non_discr}.
\\We can observe that cdkm is always better than baselines, whatever the method 
to define X' and the type of pretraining. We also observe that Lexical Pretrain 
is generally better than Simple Pretrain. Finally, the results for the mask method
and for the sim method are similar.
\\The strongest progression is for the dataset RCV1, and the weakest progression 
is for the dataset 20 newsgroup with noise.
\\While observing results for \textbf{AE+KM mask LP} and \textbf{AE+KM sim LP},
we notice that lexical biases give better results than $K$-Means  or Deep 
$K$-Means. It is with the method sim that we have the lowest performance 
decrease.
\subsubsection{Robustness}
We can see results  in figures \ref{fig:20news}.
\\We observe that when we use only one key word the results are bad for each of 
the metrics.
\\We observe that the sim method is more stable. Indeed, from two keywords, 
peroformances increase. While for the mask method we see a peak for 3 keywords, 
then performances decrease. Moreover, we note that for the sim method, the two 
methods of pretraining have fairly similar results. While for the mask method 
there is a big difference in terms of performance.