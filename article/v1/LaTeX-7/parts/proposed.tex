\section{Proposed Method}
The idea is to learn a latent space taking into account lexical constains and
background knowledge.
\\We denote $KW = \begin{pmatrix} kw_1 & kw_2 & ... & kw_{k-1} & kw_{k}
\end {pmatrix}$
the set of of index of key words. X a document of size n,
X' the vector showing only data about key words such that :
\begin{equation*}
\forall_{i=1,2,..,n}X_i' = \left\{
\begin{array}{ll}
  X_i & \mbox{if } i \in KW \\
  0 & \mbox{Otherwise.}
\end{array}
\right.
\end{equation*}
C the corpus, N the size of C, and K the number of cluster.
\subsection{Auto-Encoder}
We denote $h_X$ the encoder output : 
\begin{equation}\label{eq:h}
  h_X = g(X,\theta_1)
\end{equation}
\subsubsection{Lexical Constraints}
To learn the latent space we use sparse penalties such that the representation
of the document X is close of the representation of X'. In this way, in the
latent the representation of document will be biased by key word. We introduce a
sparse penalty $\omega_{KW}$ for lexical constraints and use squared loss : 
\begin{equation}\label{eq:omega1}
  \omega_{KW} = \sum_{\forall{X\in C}} || h_X - g(X',\theta_1) ||_2^2 =
  \sum_{\forall{(X,X')\in X}} || h_X - h_{X'}||_2^2
\end{equation}
\subsubsection{Backgroung Knowledge}
In addition of lexical constraints we introduce background knowledge, so we need
a representation taking into account theses constraints.\\
For must-link constraints we want close representations for each pair $X_1$, $X_2$
in the set ML. We introduce a sparse penalty $\omega_{ML}$ for must-link
constraints and use squared loss :
\begin{equation}\label{eq:omegaML}
  \omega_{ML} = \sum_{\forall{(X_1,X_2)\in CL}} || h_{X_1} - h_{X_2} ||_2^2
\end{equation}
For must-link constraints we want away representations for each pair $X_1$, $X_2$
in the set ML.
We can use the hinge loss.
We introduce a sparse penalty $\omega_{CL}$ for cannot-link constraints.
\begin{equation}\label{eq:omegaCL}
  \omega_{CL} = \sum_{\forall{(X_1,X_2)\in ML}} max(0,
  \eta - || h_{X_1} - h_{X_2} ||_2^2)
\end{equation}
The sparse penalty is :
\begin{equation}\label{eq:Sparse}
  \Omega(C) = \omega_{KW} + \omega_{CL} + \omega_{ML}  
\end{equation}

Then the reconstruct function is :
\begin{equation}\label{eq:AEDK}
  L_{rec}(C, \theta_1) = \sum_{X \in C}(||X - f(g(X, \theta_1))||_2^2
\end{equation}
Finally the loss function for the Auto-Encoder is :
\begin{equation}\label{eq:AE}
  L_{SAE}(C, \theta_1) = L_{rec}(C, \theta_1) + \Omega(C)  
\end{equation}

\subsection{Deep K-Means}

\subsubsection{Clustering Loss}

For the clustering loss we can use squared loss with euclidian distance. We
denote $r = \begin{pmatrix} r_1 & r_2 & ... & r_i\end{pmatrix}$ the vector of
centroids.  
\begin{equation}\label{eq:loss_clust}
  L_{clust}(C,K) = \sum_{i=1}^K \sum_{X \in K_i} ||h_X - r_i ||_2^2 
\end{equation}
Finally the loss function is :
\begin{equation}\label{eq:loss_FINALE}
  Min~L(KW, C, K; \theta_1, \theta_2) = \epsilon_0.L_{SAE}(C, KW; \theta_1) 
  + \epsilon_1.L_{clust}(C,K)
\end{equation}

with $\epsilon_0 \geq 0$, $\epsilon_1 \geq 0$.
