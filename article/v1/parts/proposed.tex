\section{Proposed Method}
The idea is to learn a latent space taking into account lexical constraints and
background knowledge.
\\We denote $KW = \begin{pmatrix} kw_1 & kw_2 & ... & kw_{k-1} & kw_{k}
\end {pmatrix}$
the set of index of keywords. X a document of size N,
X' the vector showing only informations about keywords such that :
\begin{equation*}
\forall_{i=1,2,..,n}X_i' = \left\{
\begin{array}{ll}
  X_i & \mbox{if } i \in KW \\
  0 & \mbox{Otherwise.}
\end{array}
\right.
\end{equation*}
C the corpus, N the size of C, and K the number of cluster.
\subsection{Auto-Encoder}
We denote $h_X$ the encoder output : 
\begin{equation}\label{eq:h}
  h_X = g(X,\theta)
\end{equation}
\subsubsection{Lexical Constraints}
To learn the latent space we add a penalty such that the representation
of the document X is close of the representation of X'. In this way, in the
latent space, the representation of document will be biased by keywords.
We introduce a penalty $\omega_{KW}$ for lexical constraints and
use squared loss : 
\begin{equation}\label{eq:omega1}
  \omega_{KW} = \sum_{X \in C} || h_{X} - g(X',\theta) ||_2^2 =
  \sum_{X \in C} || h_{X} - h_{X'}||_2^2
\end{equation}
\subsubsection{Background Knowledge}
In addition of lexical constraints we introduce background knowledge, so we need
a representation taking into account these constraints.\\
For must-link constraints we want close representations for each pair $X_i$, $X_j$
in the set ML. We introduce a penalty $\omega_{ML}$ for must-link
constraints and use squared loss :
\begin{equation}\label{eq:omegaML}
  \omega_{ML} = \sum_{\forall{(X_i,X_j)\in ML}} || h_{X_i} - h_{X_j} ||_2^2
\end{equation}
For cannot-link constraints we want distant representations for each pair $X_i$,
$X_j$ in the set CL.
We can use the hinge loss.
We introduce a penalty $\omega_{CL}$ for cannot-link constraints.
\begin{equation}\label{eq:omegaCL}
  \omega_{CL} = \sum_{\forall{(X_i,X_j)\in CL}} max(0,
  \eta - || h_{X_i} - h_{X_j} ||_2^2)
\end{equation}
The penalty is :
\begin{equation}\label{eq:Sparse}
  \Omega(C) = \alpha_0\omega_{KW} + \alpha_1\omega_{CL} + \alpha_2\omega_{ML}  
\end{equation}
With hyperparameters $\alpha_0\geq 0$ ,$\alpha_1\geq 0$, $\alpha_2\geq 0$.
\\Then the reconstruct loss function is :
\begin{equation}\label{eq:AEDK}
  L_{rec}(C, \theta) = \sum_{X \in C}(||X - f(g(X, \theta))||_2^2
\end{equation}
Finally the loss function for the Auto-Encoder is :
\begin{equation}\label{eq:AE}
  L_{NonClust}(C,KW; \theta) = L_{rec}(C, \theta) + \Omega(C)  
\end{equation}

\subsection{Deep $K$-Means}

For the Deep $K$-Means, we can use the approach proposed by Moradi, Thonet and Gaussier 
\cite{Deap-K-Means} see in section 2.3.\\
For the clustering loss we can use squared loss with euclidian distance. We
denote $R = \begin{pmatrix} r_1 & r_2 & ... & r_K\end{pmatrix}$ the vector of
centroids, and S the set of cluster :.  
\begin{equation}\label{eq:loss_clust}
  L_{clust}(C, K; \theta, R) = \sum_{X \in C} ||g(X) - c(g(X, \theta); R) ||_2^2 
\end{equation}
Finally the loss function is :
\begin{equation}\label{eq:loss_FINALE}
  Min~L(KW, C, K; \theta) = L_{NonClust}(C, KW; \theta) + \lambda.L_{clust}(C,K)
\end{equation}
with $0 \leq \lambda \leq 1$.\\
For the learning algorithm we can use the Deep $K$-Means algorithm see
in section 2.3 introducing mini batch for pairwise
constraint~\ref{algo:cdkm}.
\begin{algorithm}[!h]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{Corpus C , number of clusters K, balancing parameter $\lambda$,
    scheme for $\alpha$, number of epochs T,
    number of minibatches MB , learning rate $\epsilon$}
  \Output{Auto-Encoder parameter $\theta$, cluster representative R}
  \ForEach{$\alpha = m_\alpha : M_\alpha$}{
    \ForEach{t = 1 : T}{
      \ForEach{n = 1 : MB}{
        Draw minibatch $\widetilde{C} \subseteq  C$\\
        Draw minibatch $\widetilde{C_{ml}} \subseteq  C x C$ for must-link
        constraints\\
        Draw minibatch $\widetilde{C_{cl}} \subseteq  C x C$ for cannot-link
        constraints\\
        Update ($\theta, R$) using SGD
      } 
    }  
  }
  \caption{\label{algo:cdkm}Constrained Deep $K$-Means}
\end{algorithm}
