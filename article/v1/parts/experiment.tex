\section{Experiment}

\subsection{Data}
To experiment our algoritm we use two corpus $C_1$ and $C_2$. $C_1$ contains a
set of documents related to Finance and $C_2$ contains a set of documents
related to Sport Each coprus are divided in 10 classes.

\subsection{Evaluation}
\subsubsection{Reference Algorithm}
We evaluate our algorithm with four algorithms :
\begin{itemize}
\item \textbf{NNclustering} proposed by Hsu and Kira
\cite{2015arXiv151106321H}.
\item \textbf{COPKmeans} proposed by Wagstaff \cite{Wagstaff:2001:CKC:645530.655669}.
\item \textbf{COPKmeans with lexical constraints}, we learn a $k$-means
  friendly space taking into account lexical constraints. The loss
  function for this model is :
  $$
  Min~L(KW, C, K; \theta) = \lambda(L_{rec}(C, \theta) + \omega_{KW} )+
  (1-\lambda)L_{clust}(C,K)
  $$
\item \textbf{Thematic kmeans}, the loss function for this algorithm is
  \begin{equation*}
    argmin \sum_{i=1}^{N}L(x_i) + \beta\sum_{k=1}^{K}||r_k - \sum_{\omega \in KW}
    \alpha_{k,\omega}h(\omega, \theta)||_2^2 + \delta\sum_{k=1}^{K}||\alpha_k|| 
  \end{equation*}
  with $L(x_i) = L_{rec}(x_i) + \lambda L_{clust}(x_i)$
\end{itemize}
\subsubsection{Metric}
To evaluate our algorithm and compare results with reference algorithms we can
use the NMI Metric, Purity Metric \cite{measure}, accuracy and adjusted
Rand index. 
\begin{itemize}
\item The NMI metric is defined by
$$
NMI(S,C) = \frac{I(S,C)}{[H(S)+H(C)]/2}
$$ 
with
$
I(S,C) =\sum_k \sum_f\frac{|s_k \cap c_f|}{N}log\frac{N|s_k \cap c_f|}{|s_k| |c_f|}
$ and $
H(S) = -\sum_k\frac{|s_k|}{N}log\frac{N|s_k|}{|s_k|}
$
\item The purity metric is defined by :
$$
purity(S,C) = \frac{1}{N}\sum_k {max}_j|s_k \cap c_j|
$$
\item The adjusted Rand index are defined by :
  $$R = \frac{a+b}{\binom{N}{2}}$$
  where a is the number of pairs of document in C
  that are in the same cluster in the predicted partition and in the
  same cluster in the real partition, and b he number of pairs of
  documents in C that are in different clusters in predicted partition
  and in different cluster in real partition.
\end{itemize}
\subsection{Separate Corpus }
With corpus $C_1$ and $C_2$ we generate $KW_1$ the set of keywords
for $C_1$ and $KW_2$ the set of keywords for $C_2$. Then we denote $KW$
the set of keywords for clustering such that $KW = KW_1 \cup KW_2$.\\
To generate $KW_1$ (resp $KW_2$) we extract keywords from $C_1$ (resp $C_2$)
with RAKE algorithm \cite{rake}.\\
For must-link constraints we extract a set of pairs $(X_1, X_2)$ from each
corpus such that $(X_1, X_2) \in C_1 \vee (X_1, X_2) \in C_2$.
And for cannot-link constraints we extract a set of pairs $(X_1, X_2)$ from
each corpus such that $(X_1 \in C_1 \wedge  X_2 \in C_2)\vee (X_1 \in C_2 \wedge
X_2 \in C_1)$.\\
The purpose of the experiment is to rediscover the different corpus $C_1$ and
$C_2$ with keywords and background knowledge.
\subsection{Noising Text}
With corpus $C_1$ and $C_2$ we generate $KW$ the set of keywords
for $C_1$.
To generate $KW$ we extract keywords from $C_1$ with RAKE algorithm \cite{rake}.
For must-link constraints we extract a set of pairs $(X_1, X_2)$ from Corpus
$C_1$, with $X_1$, $X_2$ in the same class.
And for cannot-link constraints we extract a set of pairs $(X_1, X_2)$ from Corpus
$C_1$, with $X_1$, $X_2$ in different classes.
Then we concatenate documents from $C_1$ with documents from $C_2$.\\
The purpose of the experiment is to rediscover the different classes of the
corpus $C1$ with keywords and background knowledge.
