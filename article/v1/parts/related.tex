\section{Related Work}\label{sec:related}

\subsection{Auto-Encoder}
The Auto-Encoder allows to learn a latent space with significantly informations 
for the clustering without loss of information. The Auto-Encoder tries to learn 
a function $f (X, \theta) = X$. In other words, it is trying to learn an 
approximation of the identity function. An Auto-Encoder is composed in two parts
an encoder function g, and a decoder function f.
To learn the identity function, we use a reconstruct loss $L_{rec}$ using MSE :
\begin{equation*}
  L_{rec}(X, \theta) = || X - f(g(X, \theta)) ||_2^2 
\end{equation*}

\begin{figure}
  \centering
  \tikzset{every picture/.style={scale=1.6}}
  \input{parts/res/tex/neural_network_autoencoder.tex}
  \caption{Auto-Encoder}
  \label{fig:AE}
\end{figure}
\subsection{Background knowledge}
The background knowledge are pairwise constraints :
\begin{itemize}
\item \textbf{Must-link} constraint $ML$ : $(X, X') \in ML \implies $ X and X' are in the
  same cluster.
\item \textbf{Cannot-link} constraint $CL$ : $(X, X') \in CL \implies $ X and X' are in
  different cluster.
\end{itemize}
A first method to integrate pairwise constraints to the K-Means is to use the 
COP-KMeans algorithm introduced by Wagstaff\cite{Wagstaff:2001:CKC:645530.655669}.
In this algorithm, we assign each document to the closest cluster, 
which did not violate ML and CL constraints.
\begin{algorithm}[!h]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{Corpus C, must-link constraints $ML \subseteq C x C$,
    cannot-link constraints $CL \subseteq C x C$}
  \Output{Clusters $S_1, S_2, ..., S_k$}
  Let $S_1, S_2 , ..., S_k$ be the initial clusters\\
  \Repeat{Convergence}{
    \ForEach{$X_i \in C$}{
      Assign $X_i$ to closest $S_j$ such that Violate-Constraints($X_i, C_j, CL,
      ML$) is false.\\
      \If{No such clusters exists}{
        return \{\}\\
      }
    }
    \ForEach{Clusters $S_i$}{
      Update centroids.\\
    }
  }
  \Return{$S_1, S_2, ..., S_k$}
  \caption{COP-Kmeans}
\end{algorithm}

\begin{algorithm}[!h]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{Ducument X, Clusters S, must-link constraints $ML \subseteq C x C$,
    cannot-link constraints $CL \subseteq C x C$}
  \Output{True if constraints are violate, False otherwise}
  \ForEach{($X, X' \in ML$)}{
    \If{$X' \in S$}{
      \Return{True}
    }
  }
  \ForEach{($X, X' \in CL$)}{
    \If{$X' \in S$}{
      \Return{True}
    }
  }
  \Return{False}
  \caption{Violate-Constraints}
\end{algorithm}
Another method, proposed by Hsu and Kira \cite{2015arXiv151106321H}, consists to 
use a neural Network based end-to-end clustering framework. This network 
directly assigns clusters in the output layer. Then they use the softmax 
function on the output layer and the KL-Divergence to learn a latent space 
which minimizes the statistical distance between predicted cluster probabilities 
for similar pairs while maximizing the distance for dissimilar pairs

\begin{equation*}
  P = f(x_p), ~ Q = f(x_q)
\end{equation*}

\begin{equation*}
  KL(P||Q) = \sum_i^k P_ilog\frac{P_i}{Q_i}
\end{equation*}

\begin{equation*}
  I_s = \left\{
\begin{array}{ll}
  1 & \mbox{if $x_p$ and $x_q$ is a similar pair} x \in X\\
  0 & \mbox{Otherwise.}
\end{array}
\right.
\end{equation*}
%
\begin{equation*}
  I_{ds} = \left\{
\begin{array}{ll}
  1 & \mbox{if $x_p$ and $x_q$ is a disimilar pair} x \in X\\
  0 & \mbox{Otherwise.}
\end{array}
\right.
\end{equation*}
\begin{equation*}
  Loss(P || Q) = I_s(x_p, x_q)KL(P || Q) + I_{ds}(x_p, x_q)max(0,
  \eta-KL(P||Q))
\end{equation*}
\begin{equation*}
  L(P,Q) = Loss(P || Q) + Loss(Q || P)
\end{equation*}
\subsection{Deep Clustering}\label{seq:DeepClust}
A several approaches for the deep $K$-Means propose to jointy learn the
representation and perform the $K$-Means algorithm \cite{2018arXiv180107648A}.
In these approaches, network's loss are divided in two parts :
\begin{itemize}
\item \textbf{Non-Clustering loss} : The non-clustering loss doesn't
  take into of the clustering parts. It allows to learn a good
  representation for our problem.
\item \textbf{Clustering loss} : The clustering allows to learn a
  $K$-means friendly representation.
\end{itemize}
The loss function takes the form :
$$
L(X;\theta) = \lambda L_{Clust}(X,\theta) + (1-\lambda)L_{NonClust}
(X,\theta)
$$
In the method proposed by [...], they proposed to learn a  a joint DR and
$K$-means clustering approach. The loss function for this approach is :
$$ \sum_{i=1}^N ||X - f(g(X;\theta))||_2^2 + \frac{\lambda}{2}||g(X;\theta)
RS_i||_2^2$$
Where $S_{j,i} \in \{0,1\}$ is the assigment matrix.
\\ To implement SGD for updat-
ing the network parameters, we look at the problem for the data
$Xi$ :
$$ L^i = ||f(g(X_i) -X_i||_2^2 + \frac{lambda}{2}||g(X_i) -RS_i||_2^2$$
They trained the network with back-propagation based SGD. The
gradient of the above function over the network parameters is easily
computable. $\nabla_\theta L^i = \frac{\delta||f(g(X_i)) -X_i||_2^2}
{\delta\theta} + \lambda\frac{\delta||g(X_i) -X_i||_2^2}{\delta \theta}
(g(X_i) - RS_i)$. These gradient can be comput by back-propagation.\\
Then the network parameters are update by : $\theta \gets \theta -
\epsilon\nabla_\theta L^i$. The assignment matrix are updated by
\begin{equation*}
  S_{i,j} \gets \left\{
\begin{array}{ll}
  1 & \mbox{if } j = \argmin_{k = \{1 ... K\}}||g(X_i) - r_k||_2^2\\
  0 & \mbox{Otherwise.}
\end{array}
\right.
\end{equation*}
For fiwing R, they used : $r_k = (\frac{1}{C_k^i})\sum_{i \in C_k^i}
g(X_i)$ where $C_k^i$ is the recorded index set of samples assigned to
cluster k from the first sample to the current object i.
$r_k$ are updated by a simple gradient step
$$r_k \gets (\frac{1}{C_k^i})(g(X_i)-r_k)S_{k,i}$$
where the gradient step size $\frac{1}{C_k^i}$ controls the learning
rate.
\begin{algorithm}[!h]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{T : the number of epochs}
  \For{t = 1 : T }{
    Update network parameters\\
    Update assignment matrix\\
    Update centroids\\
  }
  \caption{SGD}
\end{algorithm}
