\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm} 
\usepackage{url}
\usepackage{xspace}
\usepackage[left=20mm,top=20mm]{geometry}
\usepackage{subcaption}
\usepackage{mathpazo}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage[]{algorithm2e}
\usepackage[
  style=numeric,
  natbib=true,
  sortcites=true,
  block=space]{biblatex} 
\bibliography{ressources/biblio/biblio}

\newcommand{\ie}{ie}
\newcommand{\eg}{eg}
\newcommand{\reffig}[1]{Figure~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}


\title{Integrate lexical constraints to K-Means : ML \& CL Method}
\author{Grand Maxence}
\date{\today}

\begin{document}

\maketitle
\justify

\section{The proposed Method}

The idea is to produce must-link and cannot-link constraints using
TFIDF on Key Words and Auto-Encoder.\\ \\
First, we use an Auto-Encoder to learn a latent
space where each coponents shows the TF-IDF of the Key Words without
loss of information.\\Then, we can use the idea proposed
by Yen-Chang Hsu and Zsolt Kira \cite{2015arXiv151106321H}.
We use the softmax function for having a distribution over key works
and recognise the signficiance of each key word for all documents.
And then, we use the KL Divergence to produce constraints.\\Finally,
we can use the COP-Kmeans to perform the K-Means with must-link and
cannot-link constraints in the latent space
\cite{Wagstaff:2001:CKC:645530.655669} \cite{2016arXiv161004794Y}.
\\ \\
We denote $KW = \begin{pmatrix} kw_1 & kw_2 & ... & kw_{k-1} & kw_{k}
\end {pmatrix}$
the set of key words. $W_i$ the word of index i.X a document,
Y the vector showing the TFIDF each key words, X' the representation of the
document X in the latent space. $\forall_{i=1, 2, .., k}~y_i = TFIDF(kw_i~in~X)$.
C the corpus of document and N the size of C.

\section{TFIDF}

The term frequency-inversed document frequency (TFIDF) is a method of
weghting depicting the significiance of each word in a document rather
a corpus.
\\
To compute the Term Frequency (tf) of a term t in document c we use
the double normalisation K, with k=0.5 :  
\begin{equation}\label{eq:tf}
  tf(t, c) = 0.5 + 0.5.\frac{f_{t,c}}{max_{\{t' \in c \}}f_{t',c}}
\end{equation}

\begin{equation}\label{eq:idf}
  idf(t, C) = log(\frac{N}{| \{ c \in C : t \in c \}  |})
\end{equation}

\begin{equation}\label{eq:tfidf}
  tfidf(t,c,C) = tf(t,c) - idf(t,C)
\end{equation}


\section{Learn the latent space}

The auto-encoder allows to learn a latent space with significiantly
informations for the clustering without loss of information.
We use a sparse auto-encoder for learn the latent space of document.
\\
We denote $h=f(X, \theta_1)$ the encoder output, $g(f(X, \theta_1))$
the decoder output and $\omega(h)$ the sparsity penalty, $L_{sparse}$
the function loss of the sparse auto-encoder. we use MSE for
the reconstruct loss.
\\
We can see the Auto-Encoder in figure~\ref{fig:archi}.
\begin{equation}\label{eq:omega}
  \omega(h) = || h - Softmax(Y)||_2^2
\end{equation}

\begin{equation}\label{eq:Sparse}
  L_{SAE}(X, \theta_1) = ||X - f(g(X, \theta_1))||_2^2 + \omega(h)  
\end{equation}

\begin{figure}[!t]
  \centering
  \tikzset{every picture/.style={scale=1.5}}
  \input{ressources/tex_file/neural_network_autoencoder.tex}
  \caption{Auto-Encoder}
  \label{fig:archi}
\end{figure}

\section{Produce constraints}

We consider that two documents are in the same cluster only if the
importance of key words are similare, and two documents are not in the
same cluster only if the importance of each key words are dissimilar.
\\
We can use the method proposed by Yen-Chang Hsu and Zsolt Kira
\cite{2015arXiv151106321H}. We apply the softmax function to the
encode output layer. The outputs of the whole sofmax layer could be viewed as
the distribution of the importance of each key words. We can use the
Kullback-Leibler (KL) divergence to ealuate the similarity between
distributions.
\\
To produce constraints we use a multi layer perceptron taking two
documents $x_p'$ and $x_q'$ for input and the functions $I_s$ and
$I_{ds}$ for output.
\\
We can see the network in figure~\ref{fig:final_archi}.
\begin{equation}
  X_p' = g(X_p)
\end{equation}
\begin{equation}
  X_q' = g(X_q)
\end{equation}

\begin{equation}\label{eq:Is}
I_s(X_p', X_q'; \theta_2) = \left\{
    \begin{array}{ll}
        1~if~X_p'~and~X_q'~are~similare \\
        0~otherwise
    \end{array}
\right.
\end{equation}
\begin{equation}\label{eq:Ids}
I_{ds}(X_p', X_q'; \theta_2) = \left\{
    \begin{array}{ll}
        1~if~X_p'~and~X_q'~are~disimilare \\
        0~otherwise
    \end{array}
    \right.
\end{equation}

\begin{equation}\label{eq:KL}
KL(X_p' || X_q') = \sum_{i=1}^k {X_p'}_i . log(\frac{{X_p'}_i}{{X_q'}_i}) 
\end{equation}

We denote $L_{KL}$ the loss function for this network :
\begin{equation}\label{eq:LossKL}
L_{KL}(X_p', X_q') = L(X_p' || X_q') + L(X_q' || X_p') 
\end{equation}
\begin{equation}\label{eq:KLpq}
  L(X_p' || X_q') = I_s(X_p', X_q'; \theta_2) . KL(X_p' || X_q') +
  I_{ds}(X_p', X_q''; \theta_2) . min(0, margin - KL(X_p' || X_q'))
\end{equation}
\\ \\
Finally the loss function is :

\begin{equation}\label{eq:loss_FINALE}
  L = L_{SAE}(X_P, X_q) + L_{KL}(X'_P, X'_q)
\end{equation}

\begin{figure}[!t]
  \centering
  \tikzset{every picture/.style={scale=1.5}}
  \input{ressources/tex_file/net_prod.tex}
  \caption{Produce Constraints}
  \label{fig:final_archi}
\end{figure}

\section{Integrate ML \& CL to K-Means}

We can use the COP-Kmeans algorithm proposed by Kiri Wagstaff, Claire
Cardie, Seth Rogers and Stephan Schr\"odl
\cite{Wagstaff:2001:CKC:645530.655669} 

\begin{algorithm}[H]
  \KwData{Data Set D, must-link consytraints $Con_=$, cannot-
    link constraints $Con_{\neq}$}
  Let $C_1$ ... $C_k$ be the initial clusters centers\\
  \Repeat{convergence}{
    \ForEach{$d_i \in D$}{
      Assign $d_i$ to the cluster $C_j$ such that
      violate-constraints($d_i, C_j, Con_=, Con_{\neq}$) return false 
    }
    \ForEach{$C_i$}{
      update($C_i$)
    }
  }
  return \{$C_1$...$C_k$\}
  \caption{COP-Kmeans}
\end{algorithm} 
\begin{algorithm}[H]
  \KwData{data point $d$, cluster C, must-link consytraints $Con_=$, cannot-
    link constraints $Con_{\neq}$}
    \ForEach{$d, d_= \in Con_=$}{
      \If{$d_= \notin C$}{return True} 
    }
    \ForEach{$d, d_\neq \in Con_{\neq}$}{
      \If{$d_\neq \notin C$}{return True} 
    }
  return false
  \caption{violate-constraints}
\end{algorithm} 

\nocite{*}
\printbibliography[title=References]
\end{document}
