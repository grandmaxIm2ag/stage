\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm} 
\usepackage{url}
\usepackage{xspace}
\usepackage[left=20mm,top=20mm]{geometry}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{mathpazo}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage[
  style=numeric,
  natbib=true,
  sortcites=true,
  block=space]{biblatex} 
\bibliography{ressources/biblio/biblio}

\newcommand{\ie}{ie}
\newcommand{\eg}{eg}
\newcommand{\reffig}[1]{Figure~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\title{Integrate lexical constraints to K-Means : ML \& CL Method}
\author{Grand Maxence}
\date{\today}

\begin{document}

\maketitle
\justify

\section{The proposed Method}

The idea is to produce must-link and cannot-link constraints using
TFIDF on key words and Auto-Encoder.\\ \\
First, we use an Auto-Encoder to learn a latent space where each
coponents shows the TF-IDF of the Key Words without taking account of
all other words.\\Then, we can use the idea proposed by Yen-Chang Hsu
and Zsolt Kira \cite{2015arXiv151106321H}.We use the softmax function
for having a distribution over key works andrecognise the signficiance
of each key word for all documents. And then, we usethe KL Divergence
to produce constraints.\\Finally, we can use the COP-Kmeans to perform
the K-Means with must-link andcannot-link constraints in the latent
space\cite{Wagstaff:2001:CKC:645530.655669}\cite{2016arXiv161004794Y}.
\\ \\
We denote $KW = \begin{pmatrix} kw_1 & kw_2 & ... & kw_{k-1} & kw_{k}
\end {pmatrix}$
the set of key words. $W_i$ the word of index i.X a document including
only key word such that $\forall_{i = 1,2,..,n}~x_i = 1_{KW}(W_i)$.

Y the vector of occurence of each key words in document X such that
$\forall_{i=1, 2, .., k}~y_i = occur(kw_i~in~X)$. X' the representation of
the document X in the latent space. C the corpus of document and N the
size of C.

\section{TFIDF}

The term frequency-inversed document frequency (TFIDF) is a method of
weghting depicting the significiance of each word in a document rather
a corpus.
\\
To compute the Term Frequency (tf) of a term t in document c we use
the double normalisation K, with k=0.5 :  
\begin{equation}\label{eq:tf}
  tf(t, c) = 0.5 + 0.5.\frac{f_{t,c}}{max_{\{t' \in c \}}f_{t',c}}
\end{equation}

\begin{equation}\label{eq:idf}
  idf(t, C) = log(\frac{N}{| \{ c \in C : t \in c \}  |})
\end{equation}

\begin{equation}\label{eq:tfidf}
  tfidf(t,c,C) = tf(t,c) - idf(t,C)
\end{equation}


\section{The Auto-Encoder}

We can see the architecture in figure~\ref{fig:archi}.

\subsection{Learn the Latent Space}
\begin{equation}\label{eq:tf-idf}
  L_{AE}= \lambda\sum_{x_i \in X} (x_i - AE(x_i)) +
  \beta\sum_{x_i' in X'} (x_i' - TFIDF(y_i))
\end{equation}
\begin{equation}\label{eq:lambda}
  \lambda = ....
\end{equation}
\begin{equation}\label{eq:beta}
  \beta = ....
\end{equation}

\section{Produce constraints}

We consider that two documents are in the same cluster only if the
importance of key words are similare, and two documents are not in the
same cluster only if the importance of each key words are dissimilar.
\\

We can use the method proposed by Yen-Chang Hsu and Zsolt Kira
\cite{2015arXiv151106321H}. Weapplied the softmax function to the
latent space. The outputs of the whole sofmax layer could be viewed as
the distribution of the importance of each key words. We can use the
Kullback-Leibler (KL) divergence to ealuate the similarity between
distributions. 
\\ \\
Finally the loss function is :

\begin{equation}\label{eq:loss_FINALE}
  L = L_{AE}(X_P, X_q) + L_{KL}(X_P, X_q)
\end{equation}

\begin{figure}[!t]
  \centering
  \tikzset{every picture/.style={scale=0.65}}
  \input{ressources/tex_file/neural_network_autoencoder.tex}
  \caption{Auto-Encoder}
  \label{fig:archi}
\end{figure}

\section{Integrate ML \& CL to K-Means}

\nocite{*}
\printbibliography[title=References]
\end{document}
